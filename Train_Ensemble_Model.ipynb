{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a559766-03b5-49b5-8440-36be5a7f2807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dba2278-ac16-47f4-8f05-eaeb0967e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint -> result 불러오기\n",
    "import argparse\n",
    "from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
    "from exp.exp_imputation import Exp_Imputation\n",
    "from exp.exp_short_term_forecasting import Exp_Short_Term_Forecast\n",
    "from exp.exp_long_term_forecasting_partial import Exp_Long_Term_Forecast_Partial\n",
    "from exp.exp_anomaly_detection import Exp_Anomaly_Detection\n",
    "from exp.exp_classification import Exp_Classification\n",
    "from utils.print_args import print_args\n",
    "import random\n",
    "import time\n",
    "from utils.metrics import *\n",
    "from utils.tools import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4e6687-5c08-4ae4-b322-14474d098b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "parser = argparse.ArgumentParser(description='TimesNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd6ab229-a7d8-4cad-8eae-4866ff3bbbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--base_model'], dest='base_model', nargs=None, const=None, default='iTransformer', type=<class 'str'>, choices=None, help='Base Model Type', metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config 지정 -> 오류 방지를 위해 \n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--task_name', type=str, default='long_term_forecast',\n",
    "                    help='task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]')\n",
    "parser.add_argument('--is_training', type=int, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Transformer, TimesNet]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# added option \n",
    "parser.add_argument('--train_ratio', type=float, default=0.7, help='train data ratio')\n",
    "parser.add_argument('--test_ratio', type=float, default=0.2, help='test data ratio')\n",
    "parser.add_argument('--train_step', type=float, default=1.0, help='train data with certain stes. for example train_step=2 means only train even number of data')\n",
    "\n",
    "# SparseTSF\n",
    "parser.add_argument('--period_len', type=int, default=24, help='period length')\n",
    "\n",
    "# PITS\n",
    "parser.add_argument('--fc_dropout', type=float, default=0.05, help='fully connected dropout')\n",
    "parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')\n",
    "parser.add_argument('--patch_len', type=int, default=16, help='patch length')\n",
    "parser.add_argument('--stride', type=int, default=8, help='stride')\n",
    "parser.add_argument('--shared_embedding', type=int, default=1, help='stride')\n",
    "parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')\n",
    "parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')\n",
    "parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')\n",
    "parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')\n",
    "parser.add_argument('--decomposition', type=int, default=0, help='decomposition; True 1 False 0')\n",
    "parser.add_argument('--kernel_size', type=int, default=25, help='decomposition-kernel')\n",
    "parser.add_argument('--individual', type=int, default=0, help='individual head; True 1 False 0')\n",
    "\n",
    "# Piformer\n",
    "parser.add_argument('--joint_var', type=int, default=0, help='use attention for each patching; True 1 False 0')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "parser.add_argument('--seasonal_patterns', type=str, default='Monthly', help='subset for M4')\n",
    "parser.add_argument('--inverse', action='store_true', help='inverse output data', default=False)\n",
    "\n",
    "# inputation task\n",
    "parser.add_argument('--mask_rate', type=float, default=0.25, help='mask ratio')\n",
    "\n",
    "# anomaly detection task\n",
    "parser.add_argument('--anomaly_ratio', type=float, default=0.25, help='prior anomaly ratio (%)')\n",
    "\n",
    "# model define\n",
    "parser.add_argument('--expand', type=int, default=2, help='expansion factor for Mamba')\n",
    "parser.add_argument('--d_conv', type=int, default=4, help='conv kernel size for Mamba')\n",
    "parser.add_argument('--top_k', type=int, default=5, help='for TimesBlock')\n",
    "parser.add_argument('--num_kernels', type=int, default=6, help='for Inception')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--channel_independence', type=int, default=1,\n",
    "                    help='0: channel dependence 1: channel independence for FreTS model')\n",
    "parser.add_argument('--decomp_method', type=str, default='moving_avg',\n",
    "                    help='method of series decompsition, only support moving_avg or dft_decomp')\n",
    "parser.add_argument('--use_norm', type=int, default=1, help='whether to use normalize; True 1 False 0')\n",
    "parser.add_argument('--down_sampling_layers', type=int, default=0, help='num of down sampling layers')\n",
    "parser.add_argument('--down_sampling_window', type=int, default=1, help='down sampling window size')\n",
    "parser.add_argument('--down_sampling_method', type=str, default=None,\n",
    "                    help='down sampling method, only support avg, max, conv')\n",
    "parser.add_argument('--seg_len', type=int, default=48,\n",
    "                    help='the length of segmen-wise iteration of SegRNN')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='MSE', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "\n",
    "# de-stationary projector params\n",
    "parser.add_argument('--p_hidden_dims', type=int, nargs='+', default=[128, 128],\n",
    "                    help='hidden layer dimensions of projector (List)')\n",
    "parser.add_argument('--p_hidden_layers', type=int, default=2, help='number of hidden layers in projector')\n",
    "\n",
    "# metrics (dtw)\n",
    "parser.add_argument('--use_dtw', type=bool, default=False, \n",
    "                    help='the controller of using dtw metric (dtw is time consuming, not suggested unless necessary)')\n",
    "\n",
    "# Augmentation\n",
    "parser.add_argument('--augmentation_ratio', type=int, default=0, help=\"How many times to augment\")\n",
    "parser.add_argument('--seed', type=int, default=2, help=\"Randomization seed\")\n",
    "parser.add_argument('--jitter', default=False, action=\"store_true\", help=\"Jitter preset augmentation\")\n",
    "parser.add_argument('--scaling', default=False, action=\"store_true\", help=\"Scaling preset augmentation\")\n",
    "parser.add_argument('--permutation', default=False, action=\"store_true\", help=\"Equal Length Permutation preset augmentation\")\n",
    "parser.add_argument('--randompermutation', default=False, action=\"store_true\", help=\"Random Length Permutation preset augmentation\")\n",
    "parser.add_argument('--magwarp', default=False, action=\"store_true\", help=\"Magnitude warp preset augmentation\")\n",
    "parser.add_argument('--timewarp', default=False, action=\"store_true\", help=\"Time warp preset augmentation\")\n",
    "parser.add_argument('--windowslice', default=False, action=\"store_true\", help=\"Window slice preset augmentation\")\n",
    "parser.add_argument('--windowwarp', default=False, action=\"store_true\", help=\"Window warp preset augmentation\")\n",
    "parser.add_argument('--rotation', default=False, action=\"store_true\", help=\"Rotation preset augmentation\")\n",
    "parser.add_argument('--spawner', default=False, action=\"store_true\", help=\"SPAWNER preset augmentation\")\n",
    "parser.add_argument('--dtwwarp', default=False, action=\"store_true\", help=\"DTW warp preset augmentation\")\n",
    "parser.add_argument('--shapedtwwarp', default=False, action=\"store_true\", help=\"Shape DTW warp preset augmentation\")\n",
    "parser.add_argument('--wdba', default=False, action=\"store_true\", help=\"Weighted DBA preset augmentation\")\n",
    "parser.add_argument('--discdtw', default=False, action=\"store_true\", help=\"Discrimitive DTW warp preset augmentation\")\n",
    "parser.add_argument('--discsdtw', default=False, action=\"store_true\", help=\"Discrimitive shapeDTW warp preset augmentation\")\n",
    "parser.add_argument('--extra_tag', type=str, default=\"\", help=\"Anything extra\")\n",
    "\n",
    "#piformer\n",
    "parser.add_argument('--shuffle', type=int, default=1, help=\"Shuffle data when training\")\n",
    "parser.add_argument('--base_model', type=str, default=\"iTransformer\", help=\"Base Model Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa9d399-bb09-4e1f-bfb9-7691c88e4858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크립트 4개 정리 (./scripts/long_term_forecast/Multi_script/iTransformer_exchange_weather.sh)\n",
    "scripts_list = [\"\"\"--task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./dataset/exchange_rate/ \\\n",
    "  --data_path exchange_rate.csv \\\n",
    "  --model_id iTransformer_Exchange_96_96 \\\n",
    "  --model iTransformer \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 8 \\\n",
    "  --dec_in 8 \\\n",
    "  --c_out 8 \\\n",
    "  --batch_size 8 \\\n",
    "  --d_model 64\\\n",
    "  --d_ff 128\\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \"\"\",\n",
    "                \"\"\"--task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./dataset/exchange_rate/ \\\n",
    "  --data_path exchange_rate.csv \\\n",
    "  --model_id iTransformer_Exchange_96_192 \\\n",
    "  --model iTransformer \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 192 \\\n",
    "  --e_layers 2 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 8 \\\n",
    "  --dec_in 8 \\\n",
    "  --c_out 8 \\\n",
    "  --batch_size 8 \\\n",
    "  --d_model 64\\\n",
    "  --d_ff 128\\\n",
    "  --des 'Exp' \\\n",
    "  --itr 1 \"\"\",\n",
    "                \"\"\"--task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./dataset/weather/ \\\n",
    "  --data_path weather.csv \\\n",
    "  --model_id iTransformer_weather_96_96 \\\n",
    "  --model iTransformer \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 96 \\\n",
    "  --e_layers 3 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 21 \\\n",
    "  --dec_in 21 \\\n",
    "  --c_out 21 \\\n",
    "  --des 'Exp' \\\n",
    "  --batch_size 8 \\\n",
    "  --d_model 64\\\n",
    "  --d_ff 128\\\n",
    "  --itr 1\"\"\",\n",
    "                \"\"\"--task_name long_term_forecast \\\n",
    "  --is_training 1 \\\n",
    "  --root_path ./dataset/weather/ \\\n",
    "  --data_path weather.csv \\\n",
    "  --model_id iTransformer_weather_96_192 \\\n",
    "  --model iTransformer \\\n",
    "  --data custom \\\n",
    "  --features M \\\n",
    "  --seq_len 96 \\\n",
    "  --label_len 48 \\\n",
    "  --pred_len 192 \\\n",
    "  --e_layers 3 \\\n",
    "  --d_layers 1 \\\n",
    "  --factor 3 \\\n",
    "  --enc_in 21 \\\n",
    "  --dec_in 21 \\\n",
    "  --c_out 21 \\\n",
    "  --des 'Exp' \\\n",
    "  --batch_size 8 \\\n",
    "  --d_model 64\\\n",
    "  --d_ff 128\\\n",
    "  --itr 1 \"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a32461c-63f5-4665-91ba-1107067d8eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(activation='gelu', affine=0, anomaly_ratio=0.25, augmentation_ratio=0, base_model='iTransformer', batch_size=8, c_out=8, channel_independence=1, checkpoints='./checkpoints/', d_conv=4, d_ff=128, d_layers=1, d_model=64, data='custom', data_path='exchange_rate.csv', dec_in=8, decomp_method='moving_avg', decomposition=0, des=\"'Exp'\", devices='0,1,2,3', discdtw=False, discsdtw=False, distil=True, down_sampling_layers=0, down_sampling_method=None, down_sampling_window=1, dropout=0.1, dtwwarp=False, e_layers=2, embed='timeF', enc_in=8, expand=2, extra_tag='', factor=3, fc_dropout=0.05, features='M', freq='h', gpu=0, head_dropout=0.0, individual=0, inverse=False, is_training=1, itr=1, jitter=False, joint_var=0, kernel_size=25, label_len=48, learning_rate=0.0001, loss='MSE', lradj='type1', magwarp=False, mask_rate=0.25, model='iTransformer', model_id='iTransformer_Exchange_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, padding_patch='end', patch_len=16, patience=3, period_len=24, permutation=False, pred_len=96, randompermutation=False, revin=1, root_path='./dataset/exchange_rate/', rotation=False, scaling=False, seasonal_patterns='Monthly', seed=2, seg_len=48, seq_len=96, shapedtwwarp=False, shared_embedding=1, shuffle=1, spawner=False, stride=8, subtract_last=0, target='OT', task_name='long_term_forecast', test_ratio=0.2, timewarp=False, top_k=5, train_epochs=10, train_ratio=0.7, train_step=1.0, use_amp=False, use_dtw=False, use_gpu=True, use_multi_gpu=False, use_norm=1, wdba=False, windowslice=False, windowwarp=False)\n"
     ]
    }
   ],
   "source": [
    "args0 = parser.parse_args(scripts_list[0].split())\n",
    "args0.use_gpu = True if torch.cuda.is_available() and args0.use_gpu else False\n",
    "\n",
    "if args0.use_gpu and args0.use_multi_gpu:\n",
    "    args0.devices = args0.devices.replace(' ', '')\n",
    "    device_ids0 = args0.devices.split(',')\n",
    "    args0.device_ids = [int(id_) for id_ in device_ids0]\n",
    "    args0.gpu = args0.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d61cd3d5-90f2-4787-961b-29f2066d3e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(activation='gelu', affine=0, anomaly_ratio=0.25, augmentation_ratio=0, base_model='iTransformer', batch_size=8, c_out=8, channel_independence=1, checkpoints='./checkpoints/', d_conv=4, d_ff=128, d_layers=1, d_model=64, data='custom', data_path='exchange_rate.csv', dec_in=8, decomp_method='moving_avg', decomposition=0, des=\"'Exp'\", devices='0,1,2,3', discdtw=False, discsdtw=False, distil=True, down_sampling_layers=0, down_sampling_method=None, down_sampling_window=1, dropout=0.1, dtwwarp=False, e_layers=2, embed='timeF', enc_in=8, expand=2, extra_tag='', factor=3, fc_dropout=0.05, features='M', freq='h', gpu=0, head_dropout=0.0, individual=0, inverse=False, is_training=1, itr=1, jitter=False, joint_var=0, kernel_size=25, label_len=48, learning_rate=0.0001, loss='MSE', lradj='type1', magwarp=False, mask_rate=0.25, model='iTransformer', model_id='iTransformer_Exchange_96_192', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, padding_patch='end', patch_len=16, patience=3, period_len=24, permutation=False, pred_len=192, randompermutation=False, revin=1, root_path='./dataset/exchange_rate/', rotation=False, scaling=False, seasonal_patterns='Monthly', seed=2, seg_len=48, seq_len=96, shapedtwwarp=False, shared_embedding=1, shuffle=1, spawner=False, stride=8, subtract_last=0, target='OT', task_name='long_term_forecast', test_ratio=0.2, timewarp=False, top_k=5, train_epochs=10, train_ratio=0.7, train_step=1.0, use_amp=False, use_dtw=False, use_gpu=True, use_multi_gpu=False, use_norm=1, wdba=False, windowslice=False, windowwarp=False)\n"
     ]
    }
   ],
   "source": [
    "args1 = parser.parse_args(scripts_list[1].split())\n",
    "args1.use_gpu = True if torch.cuda.is_available() and args1.use_gpu else False\n",
    "\n",
    "if args1.use_gpu and args1.use_multi_gpu:\n",
    "    args1.devices = args1.devices.replace(' ', '')\n",
    "    device_ids1 = args1.devices.split(',')\n",
    "    args1.device_ids = [int(id_) for id_ in device_ids1]\n",
    "    args1.gpu = args1.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d365791-acf3-43dc-8967-c84ff257e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(activation='gelu', affine=0, anomaly_ratio=0.25, augmentation_ratio=0, base_model='iTransformer', batch_size=8, c_out=21, channel_independence=1, checkpoints='./checkpoints/', d_conv=4, d_ff=128, d_layers=1, d_model=64, data='custom', data_path='weather.csv', dec_in=21, decomp_method='moving_avg', decomposition=0, des=\"'Exp'\", devices='0,1,2,3', discdtw=False, discsdtw=False, distil=True, down_sampling_layers=0, down_sampling_method=None, down_sampling_window=1, dropout=0.1, dtwwarp=False, e_layers=3, embed='timeF', enc_in=21, expand=2, extra_tag='', factor=3, fc_dropout=0.05, features='M', freq='h', gpu=0, head_dropout=0.0, individual=0, inverse=False, is_training=1, itr=1, jitter=False, joint_var=0, kernel_size=25, label_len=48, learning_rate=0.0001, loss='MSE', lradj='type1', magwarp=False, mask_rate=0.25, model='iTransformer', model_id='iTransformer_weather_96_96', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, padding_patch='end', patch_len=16, patience=3, period_len=24, permutation=False, pred_len=96, randompermutation=False, revin=1, root_path='./dataset/weather/', rotation=False, scaling=False, seasonal_patterns='Monthly', seed=2, seg_len=48, seq_len=96, shapedtwwarp=False, shared_embedding=1, shuffle=1, spawner=False, stride=8, subtract_last=0, target='OT', task_name='long_term_forecast', test_ratio=0.2, timewarp=False, top_k=5, train_epochs=10, train_ratio=0.7, train_step=1.0, use_amp=False, use_dtw=False, use_gpu=True, use_multi_gpu=False, use_norm=1, wdba=False, windowslice=False, windowwarp=False)\n"
     ]
    }
   ],
   "source": [
    "args2 = parser.parse_args(scripts_list[2].split())\n",
    "args2.use_gpu = True if torch.cuda.is_available() and args2.use_gpu else False\n",
    "\n",
    "if args2.use_gpu and args2.use_multi_gpu:\n",
    "    args2.devices = args2.devices.replace(' ', '')\n",
    "    device_ids2 = args2.devices.split(',')\n",
    "    args2.device_ids = [int(id_) for id_ in device_ids2]\n",
    "    args2.gpu = args2.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19deefc2-4c6f-4c2e-8b77-c07907cc6bd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(activation='gelu', affine=0, anomaly_ratio=0.25, augmentation_ratio=0, base_model='iTransformer', batch_size=8, c_out=21, channel_independence=1, checkpoints='./checkpoints/', d_conv=4, d_ff=128, d_layers=1, d_model=64, data='custom', data_path='weather.csv', dec_in=21, decomp_method='moving_avg', decomposition=0, des=\"'Exp'\", devices='0,1,2,3', discdtw=False, discsdtw=False, distil=True, down_sampling_layers=0, down_sampling_method=None, down_sampling_window=1, dropout=0.1, dtwwarp=False, e_layers=3, embed='timeF', enc_in=21, expand=2, extra_tag='', factor=3, fc_dropout=0.05, features='M', freq='h', gpu=0, head_dropout=0.0, individual=0, inverse=False, is_training=1, itr=1, jitter=False, joint_var=0, kernel_size=25, label_len=48, learning_rate=0.0001, loss='MSE', lradj='type1', magwarp=False, mask_rate=0.25, model='iTransformer', model_id='iTransformer_weather_96_192', moving_avg=25, n_heads=8, num_kernels=6, num_workers=10, output_attention=False, p_hidden_dims=[128, 128], p_hidden_layers=2, padding_patch='end', patch_len=16, patience=3, period_len=24, permutation=False, pred_len=192, randompermutation=False, revin=1, root_path='./dataset/weather/', rotation=False, scaling=False, seasonal_patterns='Monthly', seed=2, seg_len=48, seq_len=96, shapedtwwarp=False, shared_embedding=1, shuffle=1, spawner=False, stride=8, subtract_last=0, target='OT', task_name='long_term_forecast', test_ratio=0.2, timewarp=False, top_k=5, train_epochs=10, train_ratio=0.7, train_step=1.0, use_amp=False, use_dtw=False, use_gpu=True, use_multi_gpu=False, use_norm=1, wdba=False, windowslice=False, windowwarp=False)\n"
     ]
    }
   ],
   "source": [
    "args3 = parser.parse_args(scripts_list[3].split())\n",
    "args3.use_gpu = True if torch.cuda.is_available() and args1.use_gpu else False\n",
    "\n",
    "if args3.use_gpu and args3.use_multi_gpu:\n",
    "    args1.devices = args3.devices.replace(' ', '')\n",
    "    device_ids3 = args3.devices.split(',')\n",
    "    args3.device_ids = [int(id_) for id_ in device_ids3]\n",
    "    args3.gpu = args3.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef8942-9aed-4165-b8a8-dad39eb0b7e2",
   "metadata": {},
   "source": [
    "## 모델 사용법\n",
    "\n",
    "* 우선 `./scripts/long_term_forecast/Multi_script/iTransformer_exchange_weather.sh` 파일을 돌린다.\n",
    "* 돌린 결과의 setting 기록을 아래 셀에 적음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09aee4a4-0256-43ca-9713-a92b4eab215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스크립트 4개 정리 (./scripts/long_term_forecast/Multi_script/iTransformer_exchange_weather.sh)\n",
    "exchange_96_96_result = \"long_term_forecast_iTransformer_Exchange_96_96_Mod-iTransformer_data-exchange_rate.csv_(96to96)_0(1727247520)\"\n",
    "exchange_96_192_result = \"long_term_forecast_iTransformer_Exchange_96_192_Mod-iTransformer_data-exchange_rate.csv_(96to192)_0(1727247761)\"\n",
    "weather_96_96_result = \"long_term_forecast_iTransformer_weather_96_96_Mod-iTransformer_data-weather.csv_(96to96)_0(1727247965)\"\n",
    "weather_96_192_result = \"long_term_forecast_iTransformer_weather_96_192_Mod-iTransformer_data-weather.csv_(96to192)_0(1727250638)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "246440a3-89be-4a64-a2b7-a0a4b06eb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변경해야 할 부분\n",
    "setting_pairs = [\n",
    "    (exchange_96_96_result, args0),\n",
    "    (exchange_96_192_result, args1),\n",
    "    (weather_96_96_result, args2),\n",
    "    (weather_96_192_result, args3)\n",
    "]\n",
    "\n",
    "idx = 0 # 순서\n",
    "setting_path = setting_pairs[idx][0]\n",
    "args = setting_pairs[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b877c08-fe8b-459e-9b97-c1588105472a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 모델 호출 - Exp_Long_Term_Forecast - exchange_96_96\n",
    "exp_model = Exp_Long_Term_Forecast(args)\n",
    "exp_model._build_model()\n",
    "device = exp_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5069374e-146d-4884-b707-b2714658eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "535e4730-4195-465a-a8ea-db45c67358bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['enc_embedding.value_embedding.weight', 'enc_embedding.value_embedding.bias', 'encoder.attn_layers.0.attention.query_projection.weight', 'encoder.attn_layers.0.attention.query_projection.bias', 'encoder.attn_layers.0.attention.key_projection.weight', 'encoder.attn_layers.0.attention.key_projection.bias', 'encoder.attn_layers.0.attention.value_projection.weight', 'encoder.attn_layers.0.attention.value_projection.bias', 'encoder.attn_layers.0.attention.out_projection.weight', 'encoder.attn_layers.0.attention.out_projection.bias', 'encoder.attn_layers.0.conv1.weight', 'encoder.attn_layers.0.conv1.bias', 'encoder.attn_layers.0.conv2.weight', 'encoder.attn_layers.0.conv2.bias', 'encoder.attn_layers.0.norm1.weight', 'encoder.attn_layers.0.norm1.bias', 'encoder.attn_layers.0.norm2.weight', 'encoder.attn_layers.0.norm2.bias', 'encoder.attn_layers.1.attention.query_projection.weight', 'encoder.attn_layers.1.attention.query_projection.bias', 'encoder.attn_layers.1.attention.key_projection.weight', 'encoder.attn_layers.1.attention.key_projection.bias', 'encoder.attn_layers.1.attention.value_projection.weight', 'encoder.attn_layers.1.attention.value_projection.bias', 'encoder.attn_layers.1.attention.out_projection.weight', 'encoder.attn_layers.1.attention.out_projection.bias', 'encoder.attn_layers.1.conv1.weight', 'encoder.attn_layers.1.conv1.bias', 'encoder.attn_layers.1.conv2.weight', 'encoder.attn_layers.1.conv2.bias', 'encoder.attn_layers.1.norm1.weight', 'encoder.attn_layers.1.norm1.bias', 'encoder.attn_layers.1.norm2.weight', 'encoder.attn_layers.1.norm2.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'projection.weight', 'projection.bias'], unexpected_keys=['a', 'b'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위의 argument와 맞는 모델 호출\n",
    "checkpoint_path = './checkpoints/'\n",
    "model_path = f\"{checkpoint_path}{setting_path}/checkpoint.pth\"\n",
    "exp_model.model.load_state_dict(torch.load(model_path), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebcf9e5f-b07a-4ba9-b6d9-53ceb35d9019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import *\n",
    "from utils.tools import linear_regression_direct, linear_predict\n",
    "from data_provider.data_factory import data_provider\n",
    "from data_provider.data_loader import Dataset_Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55269fd2-94d1-4443-ab9b-a5bf41bb237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_provider -> Exchange_rate\n",
    "dataset_exchange_96 = Dataset_Custom(args, args.root_path,\n",
    "                                    flag='train', size=(args.seq_len, args.label_len, args.pred_len),\n",
    "                                    features='M', data_path = args.data_path,\n",
    "                                    target='OT', scale=True, freq='h', timeenc=0,\n",
    "                                    seasonal_patterns=None, train_ratio=args.train_ratio, test_ratio=args.test_ratio)\n",
    "dataset_exchange_96_valid = Dataset_Custom(args, args.root_path,\n",
    "                                    flag='val', size=(args.seq_len, args.label_len, args.pred_len),\n",
    "                                    features='M', data_path = args.data_path,\n",
    "                                    target='OT', scale=True, freq='h', timeenc=0,\n",
    "                                    seasonal_patterns=None, train_ratio=args.train_ratio, test_ratio=args.test_ratio)\n",
    "dataset_exchange_96_test = Dataset_Custom(args, args.root_path,\n",
    "                                    flag='test', size=(args.seq_len, args.label_len, args.pred_len),\n",
    "                                    features='M', data_path = args.data_path,\n",
    "                                    target='OT', scale=True, freq='h', timeenc=0,\n",
    "                                    seasonal_patterns=None, train_ratio=args.train_ratio, test_ratio=args.test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcdd6605-06d2-4814-b2e8-3ee1b24879f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (enc_embedding): DataEmbedding_inverted(\n",
       "    (value_embedding): Linear(in_features=96, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (attn_layers): ModuleList(\n",
       "      (0-1): 2 x EncoderLayer(\n",
       "        (attention): AttentionLayer(\n",
       "          (inner_attention): FullAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (key_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (value_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (out_projection): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "        (conv2): Conv1d(128, 64, kernel_size=(1,), stride=(1,))\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (projection): Linear(in_features=64, out_features=96, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_model.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "333dd0c2-c436-488a-9c3c-b876461dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련셋 결과 확인하기\n",
    "from data_provider.data_factory import data_provider\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_exchange_96_loader = DataLoader(\n",
    "            dataset_exchange_96,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=False)\n",
    "dataset_exchange_96_test_loader = DataLoader(\n",
    "            dataset_exchange_96_test,\n",
    "            batch_size=1, # 모든 데이터셋을 확인해야 해서 batch_size를 강제로 1로 조정.\n",
    "            shuffle=False,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=False)\n",
    "dataset_exchange_96_valid_loader = DataLoader(\n",
    "            dataset_exchange_96_valid,\n",
    "            batch_size=1, # 모든 데이터셋을 확인해야 해서 batch_size를 강제로 1로 조정.\n",
    "            shuffle=False,\n",
    "            num_workers=args.num_workers,\n",
    "            drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eeb74f9-c2cd-4a9c-8a10-3b90c66c9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[t] for t in range(-args.seq_len, 0)])  # X는 입력 feature, shape: [seq_len, 1]\n",
    "X_new = np.array([[t] for t in range(args.pred_len)])  # 예측을 위한 새로운 시간 변수\n",
    "X_concat = np.concatenate([X, X_new], axis=0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83e45070-a191-4bf3-b434-ba865503fc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination 모델 제작\n",
    "class CombinedModel(nn.Module):\n",
    "    # 모델 정의 - \n",
    "    def __init__(self, res_A, res_B, res_C):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.res_A = res_A  # iTransformer train_result\n",
    "        self.res_B = res_B  # lin_reg_96 train_result\n",
    "        self.res_C = res_C\n",
    "        self.a = nn.Parameter(torch.ones(1, device=device)*0.998, requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.ones(1, device=device)*0.001, requires_grad=True)\n",
    "        # self.c = nn.Parameter(torch.zeros(1, device=device), requires_grad=True)\n",
    "        # self.d = nn.Parameter(torch.zeros(1, device=device), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output_A = self.res_A(x)\n",
    "        output_B = self.res_B(x)\n",
    "        output_C = self.res_C(x)\n",
    "        self.c = nn.Parameter(torch.ones(1, device=device), requires_grad=True) - self.a - self.b\n",
    "        combined_output = self.a * output_A + self.b * output_B + output_C * self.c # + self.d\n",
    "        return combined_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e38b23-9ee7-490f-ac3a-0269eed84c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "152352ab-695b-4bb5-8c9f-90eeae9e76cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_output_function\n",
    "\n",
    "def res_iTransformer(batch_x): # S \n",
    "    B, L, N = batch_x.shape  # L은 시퀀스 길이(seq_len)\n",
    "    return exp_model.model(batch_x, None, torch.zeros(B, len(X_new), N), None)\n",
    "\n",
    "def res_lin_reg(batch_x):\n",
    "    B, L, N = batch_x.shape  # L은 시퀀스 길이(seq_len)\n",
    "    # 각 배치와 변수에 대해 선형 회귀 해를 계산\n",
    "    vals = [[linear_regression_direct(X, batch_x.permute(0,2,1)[idx, var , :], device) for var in range(N)] for idx in range(B)]\n",
    "    lin_result = [[linear_predict(X_new, vals[idx][var], device) for var in range(N)] for idx in range(B)]\n",
    "    # 결과를 3D 텐서로 변환\n",
    "    lin_result = torch.stack([torch.stack(lin_result[idx], dim=0) for idx in range(B)], dim=0).to(device).permute(0,2,1)\n",
    "    return lin_result\n",
    "\n",
    "def res_lin_reg_24(batch_x):\n",
    "    B, L, N = batch_x.shape  # L은 시퀀스 길이(seq_len)\n",
    "    # 24 조각에 대해서도 계산\n",
    "    vals_24 = [[linear_regression_direct(X[-24:], batch_x.permute(0,2,1)[idx, var , -24:], device) for var in range(N)] for idx in range(B)]\n",
    "    lin_result_24 = [[linear_predict(X_new, vals_24[idx][var], device) for var in range(N)] for idx in range(B)]\n",
    "    # 결과를 3D 텐서로 변환\n",
    "    lin_result_24 = torch.stack([torch.stack(lin_result_24[idx], dim=0) for idx in range(B)], dim=0).to(device).permute(0,2,1)\n",
    "    return lin_result_24\n",
    "\n",
    "def zero_model(batch_x): # S 길이\n",
    "    B, L, N = batch_x.shape  # L은 시퀀스 길이(seq_len)\n",
    "    return torch.zeros(B, len(X_new), N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22fc20d1-e3e8-452e-b47d-2e2365a06e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100 completed\n",
      "step 200 completed\n",
      "step 300 completed\n",
      "step 400 completed\n",
      "step 500 completed\n",
      "step 600 completed\n"
     ]
    }
   ],
   "source": [
    "# 우선 train_set의 data_exchange를 바탕으로 측정값 참값 가져기\n",
    "# 트레인 데이터셋을 테스트해서 결과 받기, test 함수에서 가져옴\n",
    "preds_te_tr = [] # 예측값\n",
    "trues_te_tr = [] # 참값\n",
    "preds_te_lin = [] # 96_lin\n",
    "preds_te_lin_24 = [] # 24_lin\n",
    "\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(dataset_exchange_96_loader):\n",
    "    batch_x = batch_x.float().to(device)\n",
    "    batch_y = batch_y.float().to(device)\n",
    "\n",
    "    B, L, N = batch_x.shape  # L은 시퀀스 길이(seq_len)\n",
    "\n",
    "    batch_x_mark = batch_x_mark.float().to(device)\n",
    "    batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "    # decoder input\n",
    "    dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float()\n",
    "    dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "    # encoder - decoder\n",
    "\n",
    "    # use_amp도 사용하지 않음, \n",
    "    outputs = exp_model.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "    \n",
    "    # 각 배치와 변수에 대해 선형 회귀 해를 계산\n",
    "    vals = [[linear_regression_direct(X, batch_x.permute(0,2,1)[idx, var , :], device) for var in range(N)] for idx in range(B)]\n",
    "    lin_result = [[linear_predict(X_new, vals[idx][var], device) for var in range(N)] for idx in range(B)]\n",
    "    # 결과를 3D 텐서로 변환\n",
    "    lin_result = torch.stack([torch.stack(lin_result[idx], dim=0) for idx in range(B)], dim=0).to(device)\n",
    "\n",
    "    # 24 조각에 대해서도 계산\n",
    "    vals_24 = [[linear_regression_direct(X[-24:], batch_x.permute(0,2,1)[idx, var , -24:], device) for var in range(N)] for idx in range(B)]\n",
    "    lin_result_24 = [[linear_predict(X_new, vals_24[idx][var], device) for var in range(N)] for idx in range(B)]\n",
    "    # 결과를 3D 텐서로 변환\n",
    "    lin_result_24 = torch.stack([torch.stack(lin_result_24[idx], dim=0) for idx in range(B)], dim=0).to(device)\n",
    "    \n",
    "    outputs = outputs[:, -args.pred_len:, :]\n",
    "    batch_y = batch_y[:, -args.pred_len:, :].to(device)\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    batch_y = batch_y.detach().cpu().numpy()\n",
    "\n",
    "    pred = outputs\n",
    "    true = batch_y\n",
    "\n",
    "    preds_te_tr.append(pred)\n",
    "    trues_te_tr.append(true)\n",
    "    preds_te_lin.append(lin_result)\n",
    "    preds_te_lin_24.append(lin_result_24)\n",
    "\n",
    "    if (i+1)%100==0:\n",
    "        print(f\"step {i+1} completed\")\n",
    "    \n",
    "preds_te_tr = np.concatenate(preds_te_tr, axis=0)\n",
    "trues_te_tr = np.concatenate(trues_te_tr, axis=0)\n",
    "preds_te_lin = torch.concat(preds_te_lin, axis=0).detach().cpu().numpy()\n",
    "preds_te_lin_24 = torch.concat(preds_te_lin_24, axis=0).detach().cpu().numpy()\n",
    "preds_te_lin = np.transpose(preds_te_lin, (0,2,1))\n",
    "preds_te_lin_24 = np.transpose(preds_te_lin_24, (0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e142515-8b15-4cee-b04a-8b52ebdd59db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2203566"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(preds_te_tr, trues_te_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6fa1d-0679-4195-841c-b60389017259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b727f659-7096-4b22-8935-31561082225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 실험\n",
    "num_epochs = 5\n",
    "combine_model_test = CombinedModel(res_iTransformer, res_lin_reg, res_lin_reg_24)\n",
    "# combine_model_test training\n",
    "combine_model_test.train()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam([combine_model_test.a, combine_model_test.b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7cc52-0ff5-4da1-a584-e66a94024e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a191c0a0-7df5-480f-ac4e-6979e1197d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 데이터셋 결과 확인\n",
    "def vali(vali_data, vali_loader, criterion):\n",
    "    total_loss = []\n",
    "    combine_model_test.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            targets = batch_y[:, -args.pred_len:, :].to(device)\n",
    "            outputs = combine_model_test(batch_x)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss.append(loss)\n",
    "\n",
    "    total_loss = [v.item() for v in total_loss]\n",
    "    total_loss = np.average(total_loss)\n",
    "    combine_model_test.train()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb09000c-f1e4-46e3-9249-06f1117d5286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([0.9980], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.0010], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_model_test.a, combine_model_test.b, # combine_model_test.c # train 되지 않은 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51169a4a-e6b1-48e2-ad76-4da2d7016bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961d549-0c55-427d-99f6-5c1c78165e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50th batch done, loss 0.2042160928249359\n",
      "100th batch done, loss 0.1568441092967987\n",
      "150th batch done, loss 0.11713849008083344\n",
      "200th batch done, loss 0.07964538782835007\n",
      "250th batch done, loss 0.12236413359642029\n",
      "300th batch done, loss 0.14455156028270721\n",
      "350th batch done, loss 0.1466394066810608\n",
      "400th batch done, loss 0.1389826387166977\n",
      "450th batch done, loss 0.16542427241802216\n",
      "500th batch done, loss 0.15617336332798004\n",
      "550th batch done, loss 0.1340925097465515\n",
      "600th batch done, loss 0.07316072285175323\n",
      "==================================================\n",
      "Epoch 1 DONE\n",
      "\n",
      "vali_loss: 0.10283143683068151\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0.6943], device='cuda:0', requires_grad=True) Parameter containing:\n",
      "tensor([0.0636], device='cuda:0', requires_grad=True)\n",
      "Validation loss decreased (inf --> 0.102831).  Saving model ...\n",
      "50th batch done, loss 0.10169419646263123\n",
      "100th batch done, loss 0.12446903437376022\n",
      "150th batch done, loss 0.15075646340847015\n",
      "200th batch done, loss 0.16207049787044525\n",
      "250th batch done, loss 0.11861889809370041\n",
      "300th batch done, loss 0.11830820888280869\n",
      "350th batch done, loss 0.10288261622190475\n",
      "400th batch done, loss 0.1327972263097763\n",
      "450th batch done, loss 0.08439372479915619\n",
      "500th batch done, loss 0.09214548766613007\n",
      "550th batch done, loss 0.24701623618602753\n",
      "600th batch done, loss 0.23027551174163818\n",
      "==================================================\n",
      "Epoch 2 DONE\n",
      "\n",
      "vali_loss: 0.0950797299740498\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0.5883], device='cuda:0', requires_grad=True) Parameter containing:\n",
      "tensor([0.1816], device='cuda:0', requires_grad=True)\n",
      "Validation loss decreased (0.102831 --> 0.095080).  Saving model ...\n",
      "50th batch done, loss 0.11707484722137451\n",
      "100th batch done, loss 0.21818581223487854\n",
      "150th batch done, loss 0.3220919966697693\n",
      "200th batch done, loss 0.1114271879196167\n",
      "250th batch done, loss 0.10146427154541016\n",
      "300th batch done, loss 0.08924679458141327\n",
      "350th batch done, loss 0.1859053373336792\n",
      "400th batch done, loss 0.12735846638679504\n",
      "450th batch done, loss 0.08745592832565308\n",
      "500th batch done, loss 0.2000015377998352\n",
      "550th batch done, loss 0.2340720146894455\n",
      "600th batch done, loss 0.10029295086860657\n",
      "==================================================\n",
      "Epoch 3 DONE\n",
      "\n",
      "vali_loss: 0.0931828070652007\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0.5553], device='cuda:0', requires_grad=True) Parameter containing:\n",
      "tensor([0.2272], device='cuda:0', requires_grad=True)\n",
      "Validation loss decreased (0.095080 --> 0.093183).  Saving model ...\n",
      "50th batch done, loss 0.10995741933584213\n",
      "100th batch done, loss 0.08624903857707977\n",
      "150th batch done, loss 0.10556578636169434\n",
      "200th batch done, loss 0.10100799798965454\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "\n",
    "early_stopping = EarlyStopping(patience=2, verbose=True)\n",
    "for epoch in range(num_epochs):\n",
    "    cnt = 0\n",
    "    train_loss = []\n",
    "    # exp_model.train()\n",
    "    path = os.path.join(args.checkpoints, setting_path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(dataset_exchange_96_loader):\n",
    "        cnt += 1\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "        targets = batch_y[:, -args.pred_len:, :].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combine_model_test(batch_x)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss)\n",
    "        if (cnt+1) % 50 == 0:\n",
    "            print(f\"{cnt+1}th batch done, loss {loss}\") \n",
    "    print(\"=\"*50)\n",
    "    print(f\"Epoch {epoch+1} DONE\")\n",
    "    print()\n",
    "    train_loss = [v.item() for v in train_loss]\n",
    "    train_loss = np.average(train_loss)\n",
    "    vali_loss = vali(dataset_exchange_96_test, dataset_exchange_96_test_loader, criterion)\n",
    "    print(\"vali_loss:\", vali_loss)\n",
    "    print()\n",
    "    print(combine_model_test.a, combine_model_test.b)\n",
    "    early_stopping(vali_loss, combine_model_test, path)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    model_path = path + '/' + 'checkpoint_ensenble.pth'\n",
    "    # torch.save(combine_model_test.state_dict(), model_path)\n",
    "    # combine_model_test.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1dbf2-ec1e-49dc-abaa-9ead057b7b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_model_test.a, combine_model_test.b, # combine_model_test.c # train 되지 않은 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4117b86b-63c5-4099-b838-d1cb0f36da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 결과 도출\n",
    "combine_model_test.eval()\n",
    "combine_model_test.a, combine_model_test.b, # combine_model_test.c # train 되지 않은 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b9b83-b4f0-4991-a287-c371ca464914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터의 메트릭 결과\n",
    "MSE(preds_te_tr, trues_te_tr), MAE(preds_te_tr, trues_te_tr), SMAE(preds_te_tr, trues_te_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063f8e3-25f9-437c-9727-e308a97866e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계수 변경\n",
    "a, b, = combine_model_test.a[0].item(), combine_model_test.b[0].item(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66292f-9209-4040-b029-12d9feba92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 모델 테트트\n",
    "pred_combi = a*preds_te_tr + b*preds_te_lin + (1-a-b)*preds_te_lin_24\n",
    "MSE(pred_combi, trues_te_tr), MAE(pred_combi, trues_te_tr), SMAE(pred_combi, trues_te_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830fbfac-b124-49be-bf06-46bb9af9e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 데이터 셋 호출\n",
    "result_list = ['pred.npy', 'true.npy']\n",
    "result_path = './results/'\n",
    "np_pred = np.load(f\"{result_path}{setting_path}/{result_list[0]}\")\n",
    "np_true = np.load(f\"{result_path}{setting_path}/{result_list[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d689b-c8ba-418c-9614-4fd5c3fd4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 계산도 한다\n",
    "# 각 배치와 변수에 대해 선형 회귀 해를 계산\n",
    "B, L, N = np_pred.shape  # L은 시퀀스 길이(seq_len)\n",
    "vals = [[linear_regression_direct(X, dataset_exchange_96_test[idx][0][:, var]) for var in range(N)] for idx in range(B)]\n",
    "lin_result = [[linear_predict(X_new, vals[idx][var]) for var in range(N)] for idx in range(B)]\n",
    "# 결과를 numpy 모듈로 변경\n",
    "np_pred_lin = torch.stack([torch.stack(lin_result[idx], dim=0) for idx in range(B)], dim=0).to(device).permute(0,2,1).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2072663-144a-4cd0-bda7-da63a7671c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals2 = [[linear_regression_direct(X[-24:], dataset_exchange_96_test[idx][0][-24:, var], ) for var in range(N)] for idx in range(B)]\n",
    "lin_result2 = [[linear_predict(X_new, vals2[idx][var]) for var in range(N)] for idx in range(B)]\n",
    "# 결과를 numpy 모듈로 변경\n",
    "np_pred_lin_24 = torch.stack([torch.stack(lin_result2[idx], dim=0) for idx in range(B)], dim=0).to(device).permute(0,2,1).detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060fab4b-918f-4dca-8ec4-c732a2cddf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막으로 비교\n",
    "final_res = a*np_pred + b* np_pred_lin + (1-a-b)*np_pred_lin_24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbc696-18dc-4270-a843-8da0cb6e2773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메트릭 비교하기 (원본 iTransformer)\n",
    "MSE(np_pred, np_true), MAE(np_pred, np_true), SMAE(np_pred, np_true), REC_CORR(np_pred, np_true), STD_RATIO(np_pred, np_true), SLOPE_RATIO(np_pred, np_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21fedc2-dc57-43ed-b0de-f29395e66ed6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 메트릭 비교하기 (조합)\n",
    "MSE(final_res, np_true), MAE(final_res, np_true), SMAE(final_res, np_true), REC_CORR(final_res, np_true), STD_RATIO(final_res, np_true), SLOPE_RATIO(final_res, np_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd70ea5-0f9e-4a89-9e2c-80bf290f1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메트릭 비교하기 (lin_96)\n",
    "MSE(np_pred_lin, np_true), MAE(np_pred_lin, np_true), SMAE(np_pred_lin, np_true), REC_CORR(np_pred_lin, np_true), STD_RATIO(np_pred_lin, np_true), SLOPE_RATIO(np_pred_lin, np_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abc7ba-4c53-4c3c-9659-fb5c06183d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메트릭 비교하기 (lin_24)\n",
    "MSE(np_pred_lin_24, np_true), MAE(np_pred_lin_24, np_true), SMAE(np_pred_lin_24, np_true), REC_CORR(np_pred_lin_24, np_true), STD_RATIO(np_pred_lin_24, np_true), SLOPE_RATIO(np_pred_lin_24, np_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4076b-f696-4faf-ae5e-0c6d60847362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그리기\n",
    "idx = 474\n",
    "val = 7\n",
    "xval = range(args.pred_len)\n",
    "xval2 = range(-args.seq_len, args.pred_len)\n",
    "plt.plot(xval2, np.concatenate([np_true[idx-args.pred_len, -args.seq_len:, val], np_pred[idx, :, val]]), label='pred_normal')\n",
    "plt.plot(xval2, np.concatenate([np_true[idx-args.pred_len, -args.seq_len::, val],np_pred_lin[idx, :, val]]), label='pred_linear')\n",
    "plt.plot(xval2, np.concatenate([np_true[idx-args.pred_len, -args.seq_len::, val],np_pred_lin_24[idx, :, val]]), label='pred_linear_24')\n",
    "plt.plot(xval2, np.concatenate([np_true[idx-args.pred_len, -args.seq_len::, val],np_true[idx, :, val]]), label='true_normal')\n",
    "plt.plot(xval2, np.concatenate([np_true[idx-args.pred_len, -args.seq_len::, val],final_res[idx, :, val]]),'c', label='ensemble_normal')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c80e44-aeb1-4cd9-a091-2dbafca08a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "save_dir = f\"./results/{setting_path}/\"\n",
    "np.save(save_dir + 'pred_lin_combi.npy', final_res)\n",
    "np.save(save_dir + 'pred_lin.npy', np_pred_lin)\n",
    "np.save(save_dir + 'pred_lin24.npy', np_pred_lin_24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
